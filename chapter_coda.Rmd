# (PART) Données de composition {-}

# Introduction {#coda-introduction}
## Étendue du problème {#coda-probleme}

Soient deux archéologues, Howard et Arthur qui se partagent l'étude de plusieurs ensembles de mobilier. On considère pour l'exemple la situation idéale, où chacun à reçu une moitié identique de chaque ensemble et que tous deux sont parfaitement routiniers de ce type d'étude. Howard quantifie son matériel en distinguant les catégories A, B, C et D. De son côté Arthur ne quantifie que les catégories A, B et C (tab. \@ref(tab:coda-subcoherence)). Les résultats d'Arthur constituent ainsi une sous-composition de ceux obtenus par Howard et, en toute logique, les observations faites par les deux archéologues sur les catégories de mobilier qu'ils ont en commun devraient s'accorder.

```{r coda-subcoherence, echo=FALSE}
coda_ex <- matrix(
  data = c(0.100, 0.200, 0.100, 0.600, 0.250, 0.500, 0.250,
           0.200, 0.100, 0.100, 0.600, 0.500, 0.250, 0.250,
           0.300, 0.300, 0.200, 0.200, 0.375, 0.375, 0.250) * 100,
  nrow = 3, byrow = TRUE
)

coda_ex |> 
  as.data.frame() |> 
  knitr::kable(
    row.names = FALSE,
    col.names = c("A", "B", "C", "D", "A", "B", "C"),
    caption = "Pourcentages de céramiques appartenant à chaque catéogrie observés par deux archéologues.",
    booktabs = TRUE
  ) |> 
  kableExtra::add_header_above(c("Howard" = 4, "Arthur" = 3))
```

A partir de leurs résultats, exprimés en pourcentages, les deux archéologues cherchent ensuite à mettre en évidence les relations de dépendance entre certaines catégories de mobilier. Pour cela, tous deux calculent à partir de leurs données le coefficient de corrélation entre les catégories A et B. Howard et Arthur obtiennent `r cor(coda_ex[, 1], coda_ex[, 2])` et `r cor(coda_ex[, 5], coda_ex[, 6])`, respectivement, alors qu'ils étudient le même matériel.

## Définitions {#coda-definitions}

Une composition décrit les parties d'un tout comme des parts mutuellement exclusives et exhaustives [@aitchison1982; @aitchison1986]. Par exemple, la composition élémentaire d'une céramique correspond aux teneurs des différents oxydes la constituant (les teneurs sont exprimées en pourcentages massiques, leur somme est égale à 100%).

Une composition $x$ est donc définie comme un vecteur de longueur $D$ dont tous les **composés** ($x_{1}, x_{2}, \dotsc, x_{D}$) sont des réels strictement positifs ($x_{1} > 0, \dotsc, x_{D} > 0$) :

\begin{equation}
	x = \left[ x_{1}, x_{2}, \dotsc, x_{D} \right] \forall x_{i} > 0
  (\#eq:composition)
\end{equation}

Les composés portent une information relative : seuls les rapports entre ces composés sont d'intérêt, car la somme $k$ des teneurs du vecteur composition est arbitraire. Pour un ensemble de compositions cette somme est constante et est généralement égale à 1, 100, 10^6^, selon que l'on considère des parts d'une unité, des pourcentages ou, par exemple, des <abbr title="partie par million">ppm</abbr>, en fonction des conventions.

L'opération qui permet la normalisation d'une composition $x$ est appelée **fermeture** des données (notée $\mathscr{C}$ pour *closure*) et consiste à diviser chaque partie par le tout, puis à multiplier le résultat par une constante $k$ \@ref(eq:closure).

\begin{equation}
	\mathscr{C}(x) = \frac{k}{\sum_{i=1}^{D} x_{i}} \left[ x_{1}, x_{2}, \dotsc, x_{D} \right]
  (\#eq:closure)
\end{equation}

Par exemple, dans le cas d'un objet de masse totale $m$, décomposé en ses différents constituants de masses $m_{1}, m_{2}, \ldots, m_{D}$, on a : $x = \frac{1}{m} \left[ m_{1}, m_{2}, \dotsc, m_{D} \right]$.

Pour une composition $x$ donnée, une sous-composition $y$ est définie comme un sous-ensemble de $x$ ayant subi une opération de fermeture \@ref(eq:subcomposition).

\begin{equation}
	y = \left[ y_{1}, y_{2}, \dotsc, y_{C} \right] = \mathscr{C} \left[x_{1}, x_{2}, \dotsc, x_{C} \right] \forall C < D
  (\#eq:subcomposition)
\end{equation}

::: {.rmdnote}
En pratique, seules des sous-compositions sont généralement connues et celles-ci dépendent du problème à traiter et de la capacité à reconnaître les différents composés d'un système et à les mesurer. On ne mesure jamais l'intégralité des éléments chimiques lors d'une analyse, on ne connaît pas l'ensemble des espèces appartenant à un écosystème, certaines catégories d'objets peuvent être ignorées lors d'une étude (ou ne pas avoir été reconnues du fait de la fragmentation du matériel), etc.
:::

Du fait de la fermeture des données, l'univers d'une composition (c'est-à-dire l'ensemble de toutes les valeurs susceptibles d'être prises par ses différents composés) est un espace particulier. Ce dernier possède une dimension de moins que le nombre de composés : bien qu'il y ait $D$ observations pour chaque individus, il n'y a en effet que $D-1$ valeurs indépendantes. Autrement formulé, si les $D-1$ teneurs sont connues, sachant que leur somme est constante, la dernière teneur est connue également.

Cet univers (ou espace échantillon) peut être facilement représenté dans le cas de compositions à deux ou trois composés. Lorsque ces compositions sont projetées dans un espace réel, on constate qu'elles appartiennent toutes à un même sous-espace triangulaire (simplexe ; fig. \@ref(fig:coda-simplex))^[En termes géométriques, les observations appartiennent à un hyperplan ($S^{d}$) de l'espace euclidien à $D$ dimensions $\mathbb{R}^{D}$ (avec $d = D-1$).].

(ref:coda-simplex) Projection de compositions à deux (gauche) et trois composés (droite) dans un espace réel à deux et trois dimensions, respectivement. L'ensemble des points appartient à un sous-espace figuré en rouge. Dans le cas d'une composition à trois composés, ce sous-espace correspond au diagramme ternaire des trois composés.

```{r coda-simplex, echo=FALSE, fig.width=7, fig.height=3.5, out.width='100%', fig.cap="(ref:coda-simplex)"}
# Générer des compositions aléatoires
n <- 150
coda <- matrix(data = sample(1:99, size = 3 * n, replace = TRUE),
               nrow = n, ncol = 3, dimnames = list(NULL, c("X", "Y", "Z")))
compo2 <- as.data.frame(coda[, -3] * 100 / rowSums(coda))
compo3 <- as.data.frame(coda * 100 / rowSums(coda))

# Projeter les compositions
par(mfrow = c(1, 2), mar = c(4, 4, 1, 1) + 0.1, las = 1, cex = 1)
# Nuage de points R2
plot(
  x = compo2,
  col = "steelblue", pch = 16, type = "p",
  xlim = c(0,110), ylim = c(0,110),
  xlab = "X", ylab = "Y",
  bty = "l", xaxs = "i", yaxs = "i", asp = 1
)
polygon(
  x = c(0, 0, 100), y = c(0, 100, 0),
  border = "darkred", col = rgb(1, 0, 0, 0.1),
  lty = 1, lwd = 2
)
# Nuage de points R3
s3d <- scatterplot3d::scatterplot3d(
  x = compo3,
  color = "steelblue", pch = 16, type = "p",
  xlim = c(0,100), ylim = c(0,100), zlim = c(0,100),
  scale.y = 0.5, angle = 40, cex.axis = 1, cex.lab = 1,
  xlab = "X", ylab = "", zlab = "Z",
  mar = c(3, 2, 1, 2) + 0.1
)
text(x = 6.5, y = 0, labels = "Y", srt = 40, cex = 1.2)
polygon(
  x = s3d$xyz.convert(c(0, 100, 0), c(100, 0, 0), c(0, 0, 100)),
  border = "darkred", col = rgb(1, 0, 0, 0.1),
  lty = 1, lwd = 2
)

# Diagramme ternaire
# source("R/ternary.R")
# ternary(compo3, pch = 1, col = "steelblue",
#         col.triangle = "darkred", lwd.triangle = 2, cex.lab = 1)
```

## Principes {#coda-principes}

L'analyse de données de composition est régie par quatre principes :

*Scale invariance*
: La comparaison d'échantillons de tailles différentes est permise par l'opération de fermeture des données qui les force à partager la même somme $k$. Toute analyse de ces données doit donc conduire à des résultats similaires indépendamment de la valeur de $k$.

*Perturbation invariance*
: Toute composition peut être exprimée à l'aide d’unités différentes selon les grandeurs mesurées (kg, pourcentages massiques, dm^3^, pourcentages volumiques, moles, pression partielle, etc.). Le choix des modalités de quantification revient alors à l'expérimentateur et plusieurs approches peuvent être également considérées comme pertinentes. Les différents composants d'une composition sont qualitativement différents : chacun portant une information qui lui est propre et il peut être utile de souligner une propriété particulière^[Par exemple, lors du passage de la masse au pourcentage volumique en utilisant la densité de chaque composant.]. Malgré le changement d'unité, la composition mesurée reste la même et toute analyse statistique sensible doit permettre d'obtenir des résultats qualitativement identiques, quelle que soit l'unité choisie.

*Permutation invariance*
: Le résultat de l'analyse ne doit pas dépendre de l'ordre des composants dans le vecteur composition.

*Subcompositional coherence*
: Dans certaines situations, l'utilisation d'une sous-composition peut être pertinente et permet notamment l'inspection des données dans des espaces de moindre dimension.

Ainsi, ni la somme des composants du vecteur composition, ni le choix des unités, ni la séquence des composés ou l'étude d'un sous-ensemble issu d'une composition, ne doivent affecter les résultats de l'analyse.

Les définitions classiques de la covariance et de la corrélation ne respectent pas le principe de *subcompositional coherence* (voir l'encadré ci-après). Ce problème se manifeste particulièrement lorsque les teneurs brutes de deux composés sont représentés au sein d'un diagramme de dispersion^[Ou diagramme de Harker [@harker1909, p. 119].]. Il n'y a en effet aucune garantie que les projections à partir du jeu de données initial ou d'une sous-composition conduisent à des observations similaires. Bien que ce type de représentation soit particulièrement courant, il est impossible de faire confiance aux tendances ainsi mises en évidence [@aitchison2005]^[Voir la réponse de @cortes2009.].

La plupart des méthodes dédiées aux statistiques multivariées, dont la mise en œuvre repose sur l'exploitation des matrices de variance-covariance, sont donc inapplicables. Le recours à ces dernières requiert une transformation préalable des données, permettant de casser la contrainte de somme constante introduite par la fermeture des données.

::: {.rmdnote}
L'approche courante lorsque l'on cherche à explorer les relations de d'inter-dépendance dans le cas d'un jeu de données multivarié est de s'attacher à la [covariance](#covariance) et à la [corrélation](#correlation) entre variables. Pour deux variables aléatoires $X$ et $Y$, la covariance est en effet une mesure de la tendance de ces deux variables à évoluer dans le même sens (indiqué par le signe de la covariance), tandis que la corrélation (covariance normalisée) est une mesure de l'intensité de la relation entre ces deux variables.

Sachant les propriétés de la covariance, pour une composition $x$ de longueur $D$ telle que $x = \left[ x_{1}, x_{2}, \dotsc, x_{D} \right]$ avec $\sum_{i=1}^{D} x_{i} = 1$, on a :

\begin{equation}
  \mathrm{Cov}(x_{1}, x_{1} + \dotsb + x_{D}) = 0
  (\#eq:negbias1)
\end{equation}

Soit :

\begin{equation}
  \mathrm{Cov}(x_{1}, x_{2}) + \dotsb + \mathrm{Cov}(x_{1}, x_{D}) = - \mathrm{Var}(x_{1})
  (\#eq:negbias2)
\end{equation}

Ainsi, le membre de droite de l'équation \@ref(eq:negbias2) est toujours négatif (excepté dans le cas où le premier composé est une constante), si bien qu'au moins une des covariances du membre de gauche est forcement négative [@aitchison1986, p. 53-54]. Autrement formulé, au moins un élément de chaque ligne de la matrice de covariance doit être négatif (soit au moins $D$ éléments de la matrice).

Dans le cas des données de composition, la covariance entre deux composés est donc contrainte par la somme constante et dépend des autres composés également présents dans le jeu de données. De plus, la matrice de covariance est singulière [@vandenboogaart2013]. L'influence de la somme constante au sein de la matrice de covariance se traduit en terme de corrélation : s'agissant de compositions, le coefficient de corrélation n'est pas libre de prendre une valeur dans l'intervalle $[-1;1]$ mais correspond à une valeur arbitraire^[@pearson1896 a été le premier à relever le risque que représente l'interprétation de ce qu'il nomme alors *spurious correlation* ("corrélation fallacieuse"), mettant alors en évidence que la corrélation entre des rapports présentant des parties communes au dénominateur est arbitraire. Le problème est redécouvert au milieu du XX^e^ siècle par @chayes1960 mais il faut attendre les premiers travaux de @aitchison1982 pour qu'une solution formelle soit proposée.].

L'exemple d'une composition à deux composés $x_1$ et $x_2$ tels que $x_1 + x_2 = 1$ est particulièrement illustratif du problème [@aitchison1986, p. 54], on a en effet :

\begin{equation*}
  \mathrm{Cov}(x_{1}, x_{2}) = \mathrm{Cov}(x_{1}, 1 - x_{1}) = - \mathrm{Var}(x_{1}) = - \mathrm{Var}(x_{2})
  (\#eq:negbias3)
\end{equation*}

Soit :

\begin{equation*}
  \mathrm{Cov}(x_{1}, x_{2}) = \frac{\mathrm{Cov}(x_{1}, x_{2})}{\sqrt{\mathrm{Var}(x_{1}) \mathrm{Var}(x_{2})}} = - 1
  (\#eq:negbias4)
\end{equation*}
:::

# Transformations de données {#coda-transformations}

Sachant les contraintes propres aux données de composition, la plupart des outils statistiques et méthodes d'analyse ne peuvent être mises en œuvre sans prendre le risque d'aboutir à des inférences erronées. Néanmoins, il est possible de trouver une transformation permettant de projeter les données de compositions dans un espace réel non contraint. @aitchison1986 a montré qu'une telle transformation peut être obtenue par le logarithme de rapports de composés.

# Détection des valeurs aberrantes {#coda-outliers}

La détection de valeurs aberrantes (*outliers*), ou considérées comme inattendues, est une tâche courante lors de l'analyse de données de composition. Cette dernière repose sur la définition d'un seuil ou d'une valeur limite permettant de séparer les valeurs aberrantes du reste des données [@filzmoser2005; @reimann2005]. Ce seuil correspond généralement à une distance limite par rapport au centre du jeu de données, au-delà de laquelle les individus ne sont pas uniquement affectés par la variation naturelle du phénomène étudié, voire relèvent d'un ou de plusieurs processus complètement différents [@filzmoser2005].

Une telle approche nécessite de bien distinguer les valeurs aberrantes (observations issues d'une ou plusieurs distributions différentes) des valeurs extrêmes (issues de la même distribution malgré leur éloignement du centre) [@reimann2005]. Cette distinction constitue une des principales difficultés : les valeurs aberrantes ne sont pas nécessairement des valeurs extrêmes, en particulier lorsqu'elles résultent de phénomènes secondaires (taphonomie, contaminations...). Ainsi, les approches univariées sont généralement inefficaces pour détecter les valeurs aberrantes [@filzmoser2005]. Les données de composition sont par nature mutlivariées : la détection de valeurs abbérantes doit donc reposer sur la position des données (distance par rapport au centroïde), mais prendre également en compte la forme de ces dernières [@filzmoser2005].

La [distance de Mahalanobis](https://fr.wikipedia.org/wiki/Distance_de_Mahalanobis) \@ref(eq:mahalanobis) permet de déterminer la similarité entre un individu $x$ multivarié et un ensemble d'observations en prenant en compte la position (moyenne $\mu$), ainsi que la forme et la taille (quantifiées par la matrice de covariance $\Sigma$) de cet ensemble.

\begin{equation}
  d_M = \sqrt{ (x - \mu)^T \Sigma^{-1} (x - \mu) }
  (\#eq:mahalanobis)
\end{equation}

Dans le cas de données multivariées à $p$ dimensions distribuées normalement, la distribution du carré des distances de Mahalanobis ($d_M^2$) de l'ensemble des observations suit approximativement une loi du $\chi^2$ à $p$ degrés de liberté (fig. \@ref(fig:mahalanobis-khi2)).

(ref:mahalanobis-khi2) Gauche : données simulées distribuées normalement. Droite : histogramme des distances de Mahalanobis et densité de probabilité de la loi du $\chi^2$ à 2 degrés de liberté (en rouge).

```{r mahalanobis-khi2, fig.width=7, fig.height=3.5, fig.cap="(ref:mahalanobis-khi2)"}
## Simuler 500 observations de deux variables 
## Les deux variables ont pour moyenne 0 et écart-type 1
## La corrélation entre les deux variables est fixée à 0.8
sigma <- matrix(c(1, 0.8, 0.8, 1), nrow = 2, ncol = 2) # covariance
z <- MASS::mvrnorm(n = 500, mu = c(0, 0), Sigma = sigma)

## Calculer les distances de Mahalanobis
## (chaque observation par rapport à l'ensemble des données)
## NB : mahalanobis() retourne le carré de la distance
d2 <- mahalanobis(x = z, center = colMeans(z), cov = cov(z))

## Paramètres graphiques
par(mfrow = c(1, 2), mar = c(2, 4, 1, 1) + 0.1, las = 1)

## Diagramme de dispersion des observations
plot(x = z, type = "p", xlab = "", ylab = "", asp = 1)
abline(h = 0, v = 0, lty = 2)

## Histogramme des distances
hist(d2, freq = FALSE, main = "", xlab = "", ylab = "Densité")

## Loi du khi-deux à 2 degrés de liberté
curve(dchisq(x, df = 2), from = 0, to = max(d2),
      col = "red", lwd = 2, add = TRUE)
```

Une valeur abérrante peut ainsi être définie comme ayant une distance de Mahalanobis très importante par rapport à l'ensemble des observations. De cette façon, une certaine proportion des observations peut être isolée, par exemple : les 2% de valeurs les plus élevées, soit les valeurs supérieures au quantile d'ordre 0,98 (98^e^ centile) de la loi du $\chi^2$. Pour un seuil donné, on peut ainsi définir des ellipsoïdes ayant la même distance de Mahalanobis par rapport au centre des données (fig. \@ref(fig:mahalanobis-robuste)). Une telle approche soulève cependant quelques difficultés.

D'une part, la distance de Mahalanobis est susceptible d'être fortement affectée par la présence de valeurs aberrantes. @rousseeuw1990 recommandent ainsi d'utiliser des méthodes robustes (qui ne sont pas excessivement affectées par la présence de valeurs aberrantes) pour l'estimation de la position et de la dispersion des données^[L'ellipsoïde de volume minimal (MVE pour *minimum volume ellipsoid*) et le déterminant de covariance minimale (MCD pour *minimum covariance determinant*) sont sans doute les estimateurs les plus couramment utilisés [@filzmoser2005].]. Par exemple, la figure \@ref(fig:mahalanobis-robuste) représente la masse du cerveau de différents animaux en fonction de leur masse corporelle. Les données suivent une même tendance, à l'exception des dinosaures qui présentent une importante masse corporelle et un petit cerveau. Lorsque des estimateurs classiques sont utilisés, l'ellipse de tolérance est fortement affectée par les dinosaures. Au contraire, l'usage d'un estimateur robuste permet d'exclure les dinosaures qui constituent effectivement des valeurs aberrantes au regard de l'ensemble des données.

(ref:mahalanobis-robuste) Masse du cerveau en fonction de la masse corporelle de 28 espèces animales (échelle logarithmique, base 10). Les symboles pleins correspondent aux dinosaures. Ellipses de tolérance (seuil fixé à $\chi^2_{2;0.975}$) calculées à partir de la moyenne et de la covariance classiques (tirets) et à l'aide d'estimateurs robustes (ellipsoïde de volume minimal ; trait plein).

```{r mahalanobis-robuste, out.width='70%', fig.width=5, fig.height=5, fig.cap="(ref:mahalanobis-robuste)"}
## Répliquer la figure 1 de Rousseeuw et van Zomeren (1990)
data("Animals", package = "MASS")
brain <- log10(Animals) # Echelle logarithmique

## Paramètres graphiques
par(mar = c(4, 4, 1, 1) + 0.1, las = 1)

## Diagramme de dispersion
plot(
  x = brain, type = "p", asp = 1,
  xlim = c(-3, 7), ylim = c(-2, 6),
  xlab = "Masse corporelle (kg)",
  ylab = "Masse du cerveau (g)"
)
## Mettre en évidence les dinosaures
dino <- c("Dipliodocus", "Triceratops", "Brachiosaurus")
lines(
  x = brain[rownames(brain) %in% dino, ],
  type = "p", pch = 16
)

## Ellipses de tolérance
## Estimateurs classiques
car::ellipse(
  center = colMeans(brain),
  shape = cov(brain),
  radius = sqrt(qchisq(0.975, df = 2)),
  center.pch = NULL, col = "blue", lty = 3
)
## Estimateurs robustes
rob <- MASS::cov.rob(brain, method = "mve") # Ellipsoïde de volume minimal
car::ellipse(
  center = rob$center,
  shape = rob$cov,
  radius = sqrt(qchisq(0.975, df = 2)),
  center.pch = NULL, col = "blue", lty = 1
)
```

D'autre part, le choix du seuil permettant de caractériser une observation comme aberrante doit faire l'objet d'une discussion. Il n'y a en effet aucune raison *a priori* pour qu'un seuil particulier soit applicable à tous les jeux de donnée [@filzmoser2005]. @garrett1989 propose d'utiliser une méthode graphique en représentant les distances robustes, ordonnées par rang, en fonction des quantiles théoriques de la loi du $\chi^2$ (fig. \@ref(fig:mahalanobis-qqplot)). Les valeurs extrêmes sont alors retirées jusqu'à ce que les données s'alignent sur une même droite (fig. \@ref(fig:mahalanobis-qqplot)). Il est possible de combiner ces ces approches en fixant un seuil pour identifier de potentielles valeurs aberrantes, puis d'inspecter les données afin d'expliquer les différences observées en faisant appel à l'expérience de l'analyste^[@filzmoser2005 proposent quant à eux une méthode automatique, permettant de distinguer les valeurs extrêmes issues d'une distribution normale des valeurs aberrantes issues d'une distribution différente.].

```{r mahalanobis-qqplot, out.width='70%', fig.width=5, fig.height=5, fig.cap="(ref:mahalanobis-robuste)"}
## Distance de Mahalanobis robuste
d <- sqrt(mahalanobis(brain, center = rob$center, cov = rob$cov))
## Trier par rang
d <- sort(d)

## Paramètres graphiques
par(mar = c(4, 4, 1, 1) + 0.1, las = 1)

## Quantiles théoriques
q <- qchisq(ppoints(length(d)), df = 2)

## Diagramme Quantile-Quantile
plot(
  x = q, 
  y = d, 
  xlab = "Quantile théorique",
  ylab = "Distance de Mahalanobis robuste"
)
## Tracer la droite théorique
qqline(y = d, distribution = function(p) qchisq(p, df = 2), col = "blue")
## Mettre en évidence les dinosaures
text(x = q[d > 9], y = d[d > 9], labels = names(d[d > 9]), pos = 2)
```

Enfin, comme nous l'avons vu précédemment (§ \@ref(coda-principes)), la définition de la covariance ne respecte pas le principe de *subcompositional coherence*. Dans le cas des données de composition, une transformation préalable des données est donc nécessaire (§ \@ref(coda-transformations)). @filzmoser2008 ont montré que les distances de Mahalanobis sont les mêmes quelle que soit la transformation utilisée ($ALR$, $CLR$ ou $ILR$) si des estimateurs classiques de position et de dispersion sont utilisés. Si des estimateurs robustes sont utilisés, les distances sont les mêmes pour des données transformées $ALR$ ou $ILR$.

