# Valeurs abérantes {#sec-statistics-outliers}

La détection de valeurs aberrantes (*outliers*), ou considérées comme inattendues, est une tâche courante lors de l'analyse de données. Cette dernière repose généralement sur la définition d'un seuil ou d'une valeur limite permettant de séparer les valeurs aberrantes du reste des données [@filzmoser2005; @reimann2005]. Ce seuil correspond généralement à une distance limite par rapport au centre du jeu de données, au-delà de laquelle les individus ne sont pas uniquement affectés par la variation naturelle du phénomène étudié, voire relèvent d'un ou de plusieurs processus complètement différents [@filzmoser2005].

## Cas univarié

## Cas mutlivarié

Une telle approche nécessite de bien distinguer les valeurs aberrantes (observations issues d'une ou plusieurs distributions différentes) des valeurs extrêmes (issues de la même distribution malgré leur éloignement du centre) [@reimann2005]. Cette distinction constitue une des principales difficultés : les valeurs aberrantes ne sont pas nécessairement des valeurs extrêmes, en particulier lorsqu'elles résultent de phénomènes secondaires (taphonomie, contaminations...). Ainsi, les approches univariées sont généralement inefficaces pour détecter les valeurs aberrantes [@filzmoser2005]. Les données de composition sont par nature mutlivariées : la détection de valeurs abbérantes doit donc reposer sur la position des données (distance par rapport au centroïde), mais prendre également en compte la forme de ces dernières [@filzmoser2005].

La [distance de Mahalanobis](https://fr.wikipedia.org/wiki/Distance_de_Mahalanobis) (@eq-mahalanobis) permet de déterminer la similarité entre un individu $x$ multivarié et un ensemble d'observations en prenant en compte la position (moyenne $\mu$), ainsi que la forme et la taille (quantifiées par la matrice de covariance $\Sigma$) de cet ensemble.

$$
  d_M = \sqrt{ (x - \mu)^T \Sigma^{-1} (x - \mu) }
$$ {#eq-mahalanobis}

Dans le cas de données multivariées à $p$ dimensions distribuées normalement, la distribution du carré des distances de Mahalanobis ($d_M^2$) de l'ensemble des observations suit approximativement une loi du $\chi^2$ à $p$ degrés de liberté (@fig-mahalanobis-khi2).

```{r}
#| label: fig-mahalanobis-khi2
#| fig-width: 7
#| fig-height: 3.5
#| fig-cap: "Gauche : données simulées distribuées normalement. Droite : histogramme des distances de Mahalanobis et densité de probabilité de la loi du $\\chi^2$ à 2 degrés de liberté (en rouge)."
## Simuler 500 observations de deux variables 
## Les deux variables ont pour moyenne 0 et écart-type 1
## La corrélation entre les deux variables est fixée à 0.8
sigma <- matrix(c(1, 0.8, 0.8, 1), nrow = 2, ncol = 2) # covariance
z <- MASS::mvrnorm(n = 500, mu = c(0, 0), Sigma = sigma)

## Calculer les distances de Mahalanobis
## (chaque observation par rapport à l'ensemble des données)
## NB : mahalanobis() retourne le carré de la distance
d2 <- mahalanobis(x = z, center = colMeans(z), cov = cov(z))

## Paramètres graphiques
par(mfrow = c(1, 2), mar = c(2, 4, 1, 1) + 0.1, las = 1)

## Diagramme de dispersion des observations
plot(x = z, type = "p", xlab = "", ylab = "", asp = 1)
abline(h = 0, v = 0, lty = 2)

## Histogramme des distances
hist(d2, freq = FALSE, main = "", xlab = "", ylab = "Densité")

## Loi du khi-deux à 2 degrés de liberté
curve(dchisq(x, df = 2), from = 0, to = max(d2),
      col = "red", lwd = 2, add = TRUE)
```

Une valeur abérrante peut ainsi être définie comme ayant une distance de Mahalanobis très importante par rapport à l'ensemble des observations. De cette façon, une certaine proportion des observations peut être isolée, par exemple : les 2% de valeurs les plus élevées, soit les valeurs supérieures au quantile d'ordre 0,98 (98^e^ centile) de la loi du $\chi^2$. Pour un seuil donné, on peut ainsi définir des ellipsoïdes ayant la même distance de Mahalanobis par rapport au centre des données (@fig-mahalanobis-robuste). Une telle approche soulève cependant quelques difficultés.

D'une part, la distance de Mahalanobis est susceptible d'être fortement affectée par la présence de valeurs aberrantes. @rousseeuw1990 recommandent ainsi d'utiliser des méthodes robustes (qui ne sont pas excessivement affectées par la présence de valeurs aberrantes) pour l'estimation de la position et de la dispersion des données^[L'ellipsoïde de volume minimal (MVE pour *minimum volume ellipsoid*) et le déterminant de covariance minimale (MCD pour *minimum covariance determinant*) sont sans doute les estimateurs les plus couramment utilisés [@filzmoser2005].]. Par exemple, la @fig-mahalanobis-robuste représente la masse du cerveau de différents animaux en fonction de leur masse corporelle. Les données suivent une même tendance, à l'exception des dinosaures qui présentent une importante masse corporelle et un petit cerveau. Lorsque des estimateurs classiques sont utilisés, l'ellipse de tolérance est fortement affectée par les dinosaures. Au contraire, l'usage d'un estimateur robuste permet d'exclure les dinosaures qui constituent effectivement des valeurs aberrantes au regard de l'ensemble des données.

```{r}
#| label: fig-mahalanobis-robuste
#| out-width: '70%'
#| fig-width: 5
#| fig-height: 5
#| fig-cap: "Masse du cerveau en fonction de la masse corporelle de 28 espèces animales (échelle logarithmique, base 10). Les symboles pleins correspondent aux dinosaures. Ellipses de tolérance (seuil fixé à $\\chi^2_{2;0.975}$) calculées à partir de la moyenne et de la covariance classiques (tirets) et à l'aide d'estimateurs robustes (ellipsoïde de volume minimal ; trait plein)."
## Répliquer la figure 1 de Rousseeuw et van Zomeren (1990)
data("Animals", package = "MASS")
brain <- log10(Animals) # Echelle logarithmique

## Paramètres graphiques
par(mar = c(4, 4, 1, 1) + 0.1, las = 1)

## Diagramme de dispersion
plot(
  x = brain, type = "p", asp = 1,
  xlim = c(-3, 7), ylim = c(-2, 6),
  xlab = "Masse corporelle (kg)",
  ylab = "Masse du cerveau (g)"
)
## Mettre en évidence les dinosaures
dino <- c("Dipliodocus", "Triceratops", "Brachiosaurus")
lines(
  x = brain[rownames(brain) %in% dino, ],
  type = "p", pch = 16
)

## Ellipses de tolérance
## Estimateurs classiques
classic <- ellipse::ellipse(
  x = cov(brain),
  centre = colMeans(brain),
  t = sqrt(qchisq(0.975, df = 2))
)
lines(classic, col = "blue", lty = 3, lwd = 2)
## Estimateurs robustes
## (ellipsoïde de volume minimal)
minvol <- MASS::cov.rob(brain, method = "mve")
robust <- ellipse::ellipse(
  x = minvol$cov,
  centre = minvol$center,
  t = sqrt(qchisq(0.975, df = 2))
)
lines(robust, col = "blue", lty = 1, lwd = 2)
```

D'autre part, le choix du seuil permettant de caractériser une observation comme aberrante doit faire l'objet d'une discussion. Il n'y a en effet aucune raison *a priori* pour qu'un seuil particulier soit applicable à tous les jeux de donnée [@filzmoser2005]. @garrett1989 propose d'utiliser une méthode graphique en représentant les distances robustes, ordonnées par rang, en fonction des quantiles théoriques de la loi du $\chi^2$ (@fig-mahalanobis-qqplot). Les valeurs extrêmes sont alors retirées jusqu'à ce que les données s'alignent sur une même droite (@fig-mahalanobis-qqplot). Il est possible de combiner ces ces approches en fixant un seuil pour identifier de potentielles valeurs aberrantes, puis d'inspecter les données afin d'expliquer les différences observées en faisant appel à l'expérience de l'analyste^[@filzmoser2005 proposent quant à eux une méthode automatique, permettant de distinguer les valeurs extrêmes issues d'une distribution normale des valeurs aberrantes issues d'une distribution différente.].

```{r}
#| label: fig-mahalanobis-qqplot
#| out-width: '70%'
#| fig-width: 5
#| fig-height: 5
#| fig-cap: "Diagramme quantile-quantile des distances de Mahalanobis."
## Distance de Mahalanobis robuste
d <- sqrt(mahalanobis(brain, center = minvol$center, cov = minvol$cov))
## Trier par rang
d <- sort(d)

## Paramètres graphiques
par(mar = c(4, 4, 1, 1) + 0.1, las = 1)

## Quantiles théoriques
q <- qchisq(ppoints(length(d)), df = 2)

## Diagramme Quantile-Quantile
plot(
  x = q, 
  y = d, 
  xlab = "Quantile théorique",
  ylab = "Distance de Mahalanobis robuste"
)
## Tracer la droite théorique
qqline(y = d, distribution = function(p) qchisq(p, df = 2), col = "blue")
## Mettre en évidence les dinosaures
text(x = q[d > 9], y = d[d > 9], labels = names(d[d > 9]), pos = 2)
```
